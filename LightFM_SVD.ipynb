{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/backup/lib/python3.11/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "import pickle\n",
    "import json\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"shopping_behavior_updated.csv\")\n",
    "data['Customer ID'] = data['Customer ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age group\n",
    "data['Age Group'] = pd.cut(data['Age'], bins=[0, 25, 35, 50, 100], labels=['<25', '25-35', '35-50', '>50'])\n",
    "# Review Rating -> Postive or Negative\n",
    "data['weighted_liked'] = data['Review Rating'].apply(lambda x: 3.0 if x >= 4.0 else 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature for user and item\n",
    "user_features = [f\"Age Group:{group}\" for group in data['Age Group'].unique()] + \\\n",
    "                [f\"Gender:{gender}\" for gender in data['Gender'].unique()] + \\\n",
    "                [f\"Previous Purchases:{purchases}\" for purchases in data['Previous Purchases'].unique()]  # Thêm Previous Purchases\n",
    "\n",
    "item_features = list(data['Category'].unique()) + \\\n",
    "                list(data['Season'].unique()) + \\\n",
    "                list(data['Color'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    data['Customer ID'], \n",
    "    data['Item Purchased'], \n",
    "    user_features=user_features, \n",
    "    item_features=item_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interactions_matrix, _) = dataset.build_interactions(\n",
    "    [(x['Customer ID'], x['Item Purchased'], x['weighted_liked']) for _, x in data.iterrows()]\n",
    ")\n",
    "\n",
    "user_features_matrix = dataset.build_user_features(\n",
    "    [(x['Customer ID'], [f\"Age Group:{x['Age Group']}\", f\"Gender:{x['Gender']}\", \n",
    "                         f\"Previous Purchases:{x['Previous Purchases']}\"])  # Thêm Previous Purchases\n",
    "     for _, x in data.iterrows()]\n",
    ")\n",
    "\n",
    "item_features_matrix = dataset.build_item_features(\n",
    "    [(x['Item Purchased'], [x['Category'], x['Season'], x['Color']]) \n",
    "     for _, x in data.iterrows()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions, test_interactions = random_train_test_split(interactions_matrix, test_percentage=0.1, random_state=np.random.RandomState(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: warp, Components: 50, Epochs: 10, AUC: 0.5433\n",
      "Loss: warp, Components: 50, Epochs: 30, AUC: 0.8192\n",
      "Loss: warp, Components: 50, Epochs: 50, AUC: 0.8206\n",
      "Loss: warp, Components: 100, Epochs: 10, AUC: 0.5263\n",
      "Loss: warp, Components: 100, Epochs: 30, AUC: 0.8204\n",
      "Loss: warp, Components: 100, Epochs: 50, AUC: 0.8212\n",
      "Loss: warp, Components: 200, Epochs: 10, AUC: 0.5379\n",
      "Loss: warp, Components: 200, Epochs: 30, AUC: 0.8206\n",
      "Loss: warp, Components: 200, Epochs: 50, AUC: 0.8211\n",
      "Loss: warp, Components: 300, Epochs: 10, AUC: 0.5292\n",
      "Loss: warp, Components: 300, Epochs: 30, AUC: 0.8213\n",
      "Loss: warp, Components: 300, Epochs: 50, AUC: 0.8218\n",
      "Loss: bpr, Components: 50, Epochs: 10, AUC: 0.4998\n",
      "Loss: bpr, Components: 50, Epochs: 30, AUC: 0.5179\n",
      "Loss: bpr, Components: 50, Epochs: 50, AUC: 0.5734\n",
      "Loss: bpr, Components: 100, Epochs: 10, AUC: 0.4988\n",
      "Loss: bpr, Components: 100, Epochs: 30, AUC: 0.5254\n",
      "Loss: bpr, Components: 100, Epochs: 50, AUC: 0.5637\n",
      "Loss: bpr, Components: 200, Epochs: 10, AUC: 0.5038\n",
      "Loss: bpr, Components: 200, Epochs: 30, AUC: 0.5013\n",
      "Loss: bpr, Components: 200, Epochs: 50, AUC: 0.5544\n",
      "Loss: bpr, Components: 300, Epochs: 10, AUC: 0.5033\n",
      "Loss: bpr, Components: 300, Epochs: 30, AUC: 0.5171\n",
      "Loss: bpr, Components: 300, Epochs: 50, AUC: 0.5193\n",
      "Loss: logistic, Components: 50, Epochs: 10, AUC: 0.5025\n",
      "Loss: logistic, Components: 50, Epochs: 30, AUC: 0.5024\n",
      "Loss: logistic, Components: 50, Epochs: 50, AUC: 0.5025\n",
      "Loss: logistic, Components: 100, Epochs: 10, AUC: 0.5025\n",
      "Loss: logistic, Components: 100, Epochs: 30, AUC: 0.5025\n",
      "Loss: logistic, Components: 100, Epochs: 50, AUC: 0.5025\n",
      "Loss: logistic, Components: 200, Epochs: 10, AUC: 0.5025\n",
      "Loss: logistic, Components: 200, Epochs: 30, AUC: 0.5025\n",
      "Loss: logistic, Components: 200, Epochs: 50, AUC: 0.5025\n",
      "Loss: logistic, Components: 300, Epochs: 10, AUC: 0.5025\n",
      "Loss: logistic, Components: 300, Epochs: 30, AUC: 0.5025\n",
      "Loss: logistic, Components: 300, Epochs: 50, AUC: 0.5025\n",
      "Loss: warp-kos, Components: 50, Epochs: 10, AUC: 0.5354\n",
      "Loss: warp-kos, Components: 50, Epochs: 30, AUC: 0.8207\n",
      "Loss: warp-kos, Components: 50, Epochs: 50, AUC: 0.8204\n",
      "Loss: warp-kos, Components: 100, Epochs: 10, AUC: 0.5433\n",
      "Loss: warp-kos, Components: 100, Epochs: 30, AUC: 0.8206\n",
      "Loss: warp-kos, Components: 100, Epochs: 50, AUC: 0.8210\n",
      "Loss: warp-kos, Components: 200, Epochs: 10, AUC: 0.5291\n",
      "Loss: warp-kos, Components: 200, Epochs: 30, AUC: 0.8216\n",
      "Loss: warp-kos, Components: 200, Epochs: 50, AUC: 0.8220\n",
      "Loss: warp-kos, Components: 300, Epochs: 10, AUC: 0.5305\n",
      "Loss: warp-kos, Components: 300, Epochs: 30, AUC: 0.8215\n",
      "Loss: warp-kos, Components: 300, Epochs: 50, AUC: 0.8228\n",
      "Best configuration: Loss = warp-kos, Components = 300, Epochs = 50\n",
      "Highest AUC: 0.8228\n",
      "Model saved successfully with joblib.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# best_auc = 0\n",
    "# best_config = None\n",
    "# best_model = None\n",
    "\n",
    "# for loss in ['warp', 'bpr', 'logistic', 'warp-kos']:\n",
    "#     for components in [50, 100, 200, 300]:\n",
    "#         for epochs in [10, 30, 50]:\n",
    "#             # Initialize the model\n",
    "#             model = LightFM(loss=loss, no_components=components, random_state=42)\n",
    "            \n",
    "#             # Train the model\n",
    "#             model.fit(train_interactions, \n",
    "#                       user_features=user_features_matrix, \n",
    "#                       item_features=item_features_matrix, \n",
    "#                       epochs=epochs, \n",
    "#                       num_threads=4)\n",
    "            \n",
    "#             # Calculate AUC\n",
    "#             test_auc = auc_score(model, interactions_matrix, \n",
    "#                                  user_features=user_features_matrix, \n",
    "#                                  item_features=item_features_matrix, \n",
    "#                                  num_threads=4).mean()\n",
    "            \n",
    "#             print(f\"Loss: {loss}, Components: {components}, Epochs: {epochs}, AUC: {test_auc:.4f}\")\n",
    "            \n",
    "#             # Update the best model if this configuration performs better\n",
    "#             if test_auc > best_auc:\n",
    "#                 best_auc = test_auc\n",
    "#                 best_config = (loss, components, epochs)\n",
    "#                 best_model = model  # Save the best model\n",
    "\n",
    "# # Display the best configuration and AUC\n",
    "# print(f\"Best configuration: Loss = {best_config[0]}, Components = {best_config[1]}, Epochs = {best_config[2]}\")\n",
    "# print(f\"Highest AUC: {best_auc:.4f}\")\n",
    "\n",
    "# from joblib import dump\n",
    "\n",
    "# # Assuming `best_model` is your trained LightFM model\n",
    "# dump(best_model, 'best_lightfm_model.joblib')\n",
    "# print(\"Model saved successfully with joblib.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with joblib!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = load('best_lightfm_model.joblib')\n",
    "print(\"Model loaded successfully with joblib!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_items_with_scores(clicked_product, customer_id, model, dataset, user_features, item_features, data, top_n=10):\n",
    "    # Get the mapped index of the clicked product\n",
    "    item_index = dataset.mapping()[2][clicked_product]\n",
    "    \n",
    "    # Get the mapped index of the customer\n",
    "    user_index = dataset.mapping()[0][customer_id]\n",
    "    \n",
    "    # Predict scores for all items based on the customer and product\n",
    "    item_ids = np.arange(len(dataset.mapping()[2]))  # List of all product indices\n",
    "    scores = model.predict(\n",
    "        user_ids=user_index,\n",
    "        item_ids=item_ids,\n",
    "        user_features=user_features,\n",
    "        item_features=item_features\n",
    "    )\n",
    "    \n",
    "    # Find items similar to the clicked product using embeddings\n",
    "    _, item_embeddings = model.get_item_representations(item_features)\n",
    "    similarities = np.dot(item_embeddings, item_embeddings[item_index])\n",
    "    \n",
    "    # Filter and sort items by descending similarity\n",
    "    item_mapping = {v: k for k, v in dataset.mapping()[2].items()}\n",
    "    scored_items = [\n",
    "        (item_mapping[i], similarities[i], scores[i]) \n",
    "        for i in np.argsort(-similarities) if item_mapping[i] != clicked_product\n",
    "    ][:top_n]\n",
    "    \n",
    "    # Add additional product information\n",
    "    recommendations = [\n",
    "        (item, data[data['Item Purchased'] == item]['Category'].values[0], similarity, score)\n",
    "        for item, similarity, score in scored_items\n",
    "    ]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_users_with_scores(customer_id, model, dataset, user_features, item_features, data, top_n=10):\n",
    "    # Get the mapped index of the customer\n",
    "    user_index = dataset.mapping()[0][customer_id]\n",
    "    \n",
    "    # Retrieve user embeddings\n",
    "    _, user_embeddings = model.get_user_representations(user_features)\n",
    "    similarities = np.dot(user_embeddings, user_embeddings[user_index])\n",
    "    \n",
    "    # Find the most similar users\n",
    "    user_mapping = {v: k for k, v in dataset.mapping()[0].items()}\n",
    "    top_users = [\n",
    "        user_mapping[i] for i in np.argsort(-similarities) \n",
    "        if user_mapping[i] != customer_id\n",
    "    ][:top_n]\n",
    "    \n",
    "    # Predict scores for all items for the target user\n",
    "    item_ids = np.arange(len(dataset.mapping()[2]))\n",
    "    scores = model.predict(\n",
    "        user_ids=user_index,\n",
    "        item_ids=item_ids,\n",
    "        user_features=user_features,\n",
    "        item_features=item_features\n",
    "    ) \n",
    "    \n",
    "    # Recommend items based on similar users\n",
    "    item_mapping = {v: k for k, v in dataset.mapping()[2].items()}\n",
    "    recommendations = []\n",
    "    for similar_user in top_users:\n",
    "        similar_user_index = dataset.mapping()[0][similar_user]\n",
    "        similar_scores = model.predict(\n",
    "            user_ids=similar_user_index,\n",
    "            item_ids=item_ids,\n",
    "            user_features=user_features,\n",
    "            item_features=item_features\n",
    "        )\n",
    "        recommended_items = [\n",
    "            (item_mapping[i], scores[i]) for i in np.argsort(-similar_scores)\n",
    "        ][:top_n]\n",
    "        recommendations.extend(recommended_items)\n",
    "    \n",
    "    # Remove duplicates and sort by score\n",
    "    recommendations = list(set(recommendations))\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_recommendations(clicked_product, customer_id, model, dataset, user_features, item_features, data, top_n=10):\n",
    "    # Generate Item-based recommendations\n",
    "    item_recommendations = recommend_similar_items_with_scores(\n",
    "        clicked_product, customer_id, model, dataset, user_features, item_features, data, top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Convert Item-based recommendations into a dictionary {item: (category, similarity, score)}\n",
    "    item_recommend_dict = {\n",
    "        item: (data[data['Item Purchased'] == item]['Category'].values[0], similarity, score)\n",
    "        for item, _, similarity, score in item_recommendations\n",
    "    }\n",
    "    \n",
    "    # Generate User-based recommendations\n",
    "    user_recommendations = recommend_similar_users_with_scores(\n",
    "        customer_id, model, dataset, user_features, item_features, data, top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Merge User-based recommendations into the dictionary\n",
    "    for item, score in user_recommendations:\n",
    "        category = data[data['Item Purchased'] == item]['Category'].values[0]\n",
    "        if item in item_recommend_dict:\n",
    "            old_similarity = item_recommend_dict[item][1] if item_recommend_dict[item][1] is not None else 0\n",
    "            old_score = item_recommend_dict[item][2]\n",
    "            new_similarity = (old_similarity + 0) / 2 if old_similarity else None\n",
    "            avg_score = (old_score + score) / 2\n",
    "            item_recommend_dict[item] = (category, new_similarity, avg_score)\n",
    "        else:\n",
    "            item_recommend_dict[item] = (category, None, score) \n",
    "    \n",
    "    # Shift similarity and score values to ensure positivity\n",
    "    all_similarities = [sim for _, (_, sim, _) in item_recommend_dict.items() if sim is not None]\n",
    "    all_scores = [score for _, (_, _, score) in item_recommend_dict.items()]\n",
    "    \n",
    "    min_similarity = min(all_similarities) if all_similarities else 0\n",
    "    min_score = min(all_scores)\n",
    "    \n",
    "    for item in item_recommend_dict:\n",
    "        category, similarity, score = item_recommend_dict[item]\n",
    "        similarity = similarity - min_similarity if similarity is not None else None\n",
    "        score = score - min_score\n",
    "        item_recommend_dict[item] = (category, similarity, score)\n",
    "    \n",
    "    # Sort items by the sum of (similarity + score) in descending order\n",
    "    sorted_recommendations = sorted(\n",
    "        item_recommend_dict.items(),\n",
    "        key=lambda x: (x[1][1] or 0) + x[1][2],  # (similarity + score)\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Convert to a list of unique final recommendations\n",
    "    unique_items = set()  \n",
    "    final_recommendations = []\n",
    "\n",
    "    for item, (category, similarity, score) in sorted_recommendations:\n",
    "        if item not in unique_items:  \n",
    "            unique_items.add(item) \n",
    "            final_recommendations.append((item, category, similarity, score, (similarity or 0) + score))\n",
    "            if len(final_recommendations) >= top_n: \n",
    "                break\n",
    "    \n",
    "    return final_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product 'Sunglasses' is clicked.\n",
      "\n",
      "Similar products:\n",
      "Hat\n",
      "Jewelry\n",
      "Belt\n",
      "Handbag\n",
      "Scarf\n",
      "Backpack\n",
      "Gloves\n",
      "Jacket\n",
      "Coat\n",
      "Skirt\n"
     ]
    }
   ],
   "source": [
    "# Input the clicked product and customer ID\n",
    "clicked_product = input(\"Enter the product you clicked on (clicked product): \")\n",
    "customer_id = input(\"Enter the customer ID: \")\n",
    "\n",
    "# Generate combined recommendations\n",
    "combined_recommendation_list = combined_recommendations(\n",
    "    clicked_product, customer_id, model, dataset, user_features_matrix, item_features_matrix, data, top_n=10\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(f\"\\nProduct '{clicked_product}' is clicked.\")\n",
    "print(\"\\nSimilar products:\")\n",
    "\n",
    "# Print only the product names in the recommendation list\n",
    "for item, *_ in combined_recommendation_list:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the 'rating' column to [0, 1] using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data['normalized_rating'] = scaler.fit_transform(data[['rating']])\n",
    "\n",
    "\n",
    "# Build the Surprise Dataset using the normalized rating\n",
    "reader = Reader(rating_scale=(0, 1))  # Adjust the scale to match normalized ratings\n",
    "dataset = Dataset.load_from_df(data[[\"userID\", \"itemID\", \"normalized_rating\"]], reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "AUC: 0.5095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split into train and test sets\n",
    "trainset, testset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create and train the SVD model\n",
    "svd = SVD(n_factors=200, n_epochs=30, random_state=42, verbose=True)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "# Function to calculate AUC\n",
    "def auc_score(predictions, threshold=0.8):\n",
    "    # Separate actual and predicted values\n",
    "    y_true = [1 if pred.r_ui >= threshold else 0 for pred in predictions]\n",
    "    y_scores = [pred.est for pred in predictions]\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    return auc\n",
    "\n",
    "# Calculate AUC\n",
    "auc = auc_score(predictions, threshold=0.8)  # Threshold corresponds to ~4.0 in the original scale\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dripy Sales.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
